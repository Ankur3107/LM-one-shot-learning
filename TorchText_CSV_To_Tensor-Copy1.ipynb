{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, \n",
    "\n",
    "# The aim of this code is to convert the CSV file of\n",
    "# labels = missing words, and sentences with missing words\n",
    "# into a tensor of numbersthat can be passed through\n",
    "# the matching networks code just like the numpy array\n",
    "# used for the images in the original code based on the\n",
    "# Onmiglot dataset\n",
    "\n",
    "# The numbers in the tensor will be the numbers each word\n",
    "# refers to in the vocabulary we are building\n",
    "# We will not embed at this stage because this is\n",
    "# is done inside the matching network. We are, in effect,\n",
    "# not completing the TorchText proprocessing\n",
    "\n",
    "# This code is a mainly a mixture of two tutorials:\n",
    "# http://anie.me/On-Torchtext/\n",
    "# https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "\n",
    "# Comments are a mixture of those from the tutorials (most of them)\n",
    "# and my own\n",
    "\n",
    "# Note to self, use Conda environment PyTorch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.data import Iterator\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tackles</td>\n",
       "      <td>tadman had seven solo &lt;blank_token&gt; three assi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tackles</td>\n",
       "      <td>defensive line coach bo davis resigned his pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tackles</td>\n",
       "      <td>N and N &lt; unk &gt; were linebackers paul nelson a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tackles</td>\n",
       "      <td>courtney &lt; unk &gt; was named the sec defensive p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tackles</td>\n",
       "      <td>williams also recorded eight solo &lt;blank_token...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tackles</td>\n",
       "      <td>behind defensive mvp burt miami had one player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tackles</td>\n",
       "      <td>in addition the ecu defense ranked eleventh na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tackles</td>\n",
       "      <td>burt the other mvp accumulated nine &lt;blank_tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tackles</td>\n",
       "      <td>the other interception came from defensive bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tackles</td>\n",
       "      <td>brown finished the regular season with N &lt;blan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>amended</td>\n",
       "      <td>the constitution has since been &lt;blank_token&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>amended</td>\n",
       "      <td>the newly &lt;blank_token&gt; act did however rescin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amended</td>\n",
       "      <td>while the N constitution remains in force it h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amended</td>\n",
       "      <td>according to the ioc the existing legislation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>amended</td>\n",
       "      <td>both treaties however were &lt;blank_token&gt; to du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>amended</td>\n",
       "      <td>this was &lt;blank_token&gt; to a circle area with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amended</td>\n",
       "      <td>he neglected to add a specific name to form a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>amended</td>\n",
       "      <td>mckay &lt;blank_token&gt; his legislation to provide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>amended</td>\n",
       "      <td>when the sport resumed in N batsmen were out o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>amended</td>\n",
       "      <td>after tudman 's death in N the constitution wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                           sentence\n",
       "0   tackles  tadman had seven solo <blank_token> three assi...\n",
       "1   tackles  defensive line coach bo davis resigned his pos...\n",
       "2   tackles  N and N < unk > were linebackers paul nelson a...\n",
       "3   tackles  courtney < unk > was named the sec defensive p...\n",
       "4   tackles  williams also recorded eight solo <blank_token...\n",
       "5   tackles  behind defensive mvp burt miami had one player...\n",
       "6   tackles  in addition the ecu defense ranked eleventh na...\n",
       "7   tackles  burt the other mvp accumulated nine <blank_tok...\n",
       "8   tackles  the other interception came from defensive bac...\n",
       "9   tackles  brown finished the regular season with N <blan...\n",
       "10  amended  the constitution has since been <blank_token> ...\n",
       "11  amended  the newly <blank_token> act did however rescin...\n",
       "12  amended  while the N constitution remains in force it h...\n",
       "13  amended  according to the ioc the existing legislation ...\n",
       "14  amended  both treaties however were <blank_token> to du...\n",
       "15  amended  this was <blank_token> to a circle area with a...\n",
       "16  amended  he neglected to add a specific name to form a ...\n",
       "17  amended  mckay <blank_token> his legislation to provide...\n",
       "18  amended  when the sport resumed in N batsmen were out o...\n",
       "19  amended  after tudman 's death in N the constitution wa..."
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the data looks like\n",
    "\n",
    "pd.read_csv(\"data/train_experiments.csv\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy to define a function to \n",
    "# tokenize, or split up, into individual words\n",
    "# the labels and sentences Note the labels are already\n",
    "# individual words\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# We first define a Field, this is a class that contains\n",
    "# information on how you want the data preprocessed. It acts\n",
    "# like an instruction manual that data.TabularDataset will use.\n",
    "# We define two fields, one for the sentencesm and one for the\n",
    "# labels\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer)\n",
    "LABEL = data.Field(sequential=False, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fields know what to do when given raw data.\n",
    "# Now, we need to tell the fields what data it\n",
    "# should work on. This is where we use Datasets.\n",
    "\n",
    "# The splits method creates a dataset for the train\n",
    "# and test data by applying the same processing.\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "        path='data/', train='train_experiments.csv', test='test_experiments.csv', format='csv',\n",
    "        fields=[('label', LABEL), ('sentence', TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.dataset.TabularDataset"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchtext handles mapping words to integers, but\n",
    "# it has to be told the full range of words it should\n",
    "# handle.\n",
    "\n",
    "# We are currently building the vocab from the train\n",
    "# and test data\n",
    "\n",
    "TEXT.build_vocab(train, test)\n",
    "LABEL.build_vocab(train, test)\n",
    "\n",
    "# This makes torchtext go through all the elements in the\n",
    "# training set, check the contents corresponding to the TEXT\n",
    "# field, and register the words in its vocabulary. Torchtext\n",
    "# has its own class called Vocab for handling the vocabulary.\n",
    "# The Vocab class holds a mapping from word to id in its stoi\n",
    "# attribute and a reverse mapping in its itos attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x00000288FCB2CEA0>, {'<unk>': 0, 'amended': 1, 'tackles': 2, 'borgnine': 3, 'sir': 4, 'label': 5})\n"
     ]
    }
   ],
   "source": [
    "vocab = LABEL.vocab\n",
    "print(vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0, 'amended': 1, 'borgnine': 2, 'sir': 3, 'tackles': 4}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary\n",
    "\n",
    "{i:vocab.stoi[i] for i in vocab.stoi if i!='label'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x00000288FCB2CEA0>, {'<unk>': 0, '<pad>': 1, '<': 2, '>': 3, 'the': 4, 'unk': 5, 'blank_token': 6, 'and': 7, 'to': 8, 'N': 9, 'a': 10, 'in': 11, 'of': 12, 'was': 13, 'for': 14, 'by': 15, 'with': 16, 'defensive': 17, 'seven': 18, 'after': 19, 'had': 20, 'his': 21, 'who': 22, 'is': 23, 'player': 24, 'principal': 25, 'that': 26, 'were': 27, 'while': 28, \"'s\": 29, 'assisted': 30, 'but': 31, 'constitution': 32, 'four': 33, 'fumble': 34, 'gold': 35, 'he': 36, 'james': 37, 'one': 38, 'other': 39, 'solo': 40, 'they': 41, 'three': 42, 'up': 43, 'would': 44, 'also': 45, 'an': 46, 'as': 47, 'be': 48, 'been': 49, 'between': 50, 'burt': 51, 'coach': 52, 'coin': 53, 'dollar': 54, 'egypt': 55, 'eight': 56, 'end': 57, 'from': 58, 'gallipoli': 59, 'great': 60, 'guitar': 61, 'has': 62, 'henry': 63, 'him': 64, 'however': 65, 'i.': 66, 'it': 67, 'italy': 68, 'john': 69, 'johnson': 70, 'legislation': 71, 'loss': 72, 'macedonia': 73, 'mvp': 74, 'named': 75, 'off': 76, 'or': 77, 'palestine': 78, 'position': 79, 'recorded': 80, 'resolution': 81, 'robert': 82, 'second': 83, 'sentence': 84, 'since': 85, 'sir': 86, 'smith': 87, 'tackles': 88, 'thomas': 89, 'times': 90, 'total': 91, 'two': 92, 'which': 93, '  ': 94, '$': 95, '13th': 96, 'abandoned': 97, 'according': 98, 'accumulated': 99, 'act': 100, 'add': 101, 'addition': 102, 'adoption': 103, 'anglicus': 104, 'annular': 105, 'another': 106, 'any': 107, 'appearance': 108, 'apu': 109, 'are': 110, 'area': 111, 'army': 112, 'asia': 113, 'at': 114, 'attacked': 115, 'attacks': 116, 'azaria': 117, 'back': 118, 'batsmen': 119, 'bear': 120, 'began': 121, 'behind': 122, 'being': 123, 'best': 124, 'better': 125, 'binomial': 126, 'bo': 127, 'boasted': 128, 'borgnine': 129, 'both': 130, 'bowlers': 131, 'broke': 132, 'brooke': 133, 'brought': 134, 'brown': 135, 'came': 136, 'camp': 137, 'campers': 138, 'charter': 139, 'chris': 140, 'circle': 141, 'commented': 142, 'conducted': 143, 'consisting': 144, 'correct': 145, 'cottage': 146, 'county': 147, 'courtney': 148, 'creature': 149, 'cricket': 150, 'crushing': 151, 'dark': 152, 'davis': 153, 'death': 154, 'declaration': 155, 'defense': 156, 'deprecating': 157, 'devotion': 158, 'diameter': 159, 'did': 160, 'doing': 161, 'dominate': 162, 'double': 163, 'due': 164, 'during': 165, 'eagle': 166, 'ecu': 167, 'edwin': 168, 'eleventh': 169, 'eliminate': 170, 'enable': 171, 'episode': 172, 'ernest': 173, 'even': 174, 'existing': 175, 'explicit': 176, 'fails': 177, 'fifth': 178, 'fight': 179, 'film': 180, 'finally': 181, 'finding': 182, 'finished': 183, 'flanders': 184, 'flee': 185, 'foot': 186, 'force': 187, 'forced': 188, 'form': 189, 'fourth': 190, 'friday': 191, 'friedrich': 192, 'friend': 193, 'fumbles': 194, 'furthered': 195, 'game': 196, 'games': 197, 'garden': 198, 'gertrude': 199, 'get': 200, 'government': 201, 'greatly': 202, 'guarantees': 203, 'gulf': 204, 'hands': 205, 'hank': 206, 'have': 207, 'hell': 208, 'highest': 209, 'holl': 210, 'homer': 211, 'hunted': 212, 'idea': 213, 'if': 214, 'implied': 215, 'including': 216, 'influenced': 217, 'inswing': 218, 'interception': 219, 'into': 220, 'ioc': 221, 'ironic': 222, 'ironically': 223, 'its': 224, 'jason': 225, 'jekyll': 226, 'junior': 227, 'kg': 228, 'knife': 229, 'known': 230, 'lands': 231, 'late': 232, 'later': 233, 'law': 234, 'lbw': 235, 'led': 236, 'life': 237, 'line': 238, 'linebackers': 239, 'long': 240, 'losses': 241, 'making': 242, 'mckay': 243, 'meanwhile': 244, 'men': 245, 'miami': 246, 'milbrook': 247, 'military': 248, 'modified': 249, 'most': 250, 'mountain': 251, 'much': 252, 'name': 253, 'nationally': 254, 'ned': 255, 'needed': 256, 'neglected': 257, 'neither': 258, 'nelson': 259, 'newly': 260, 'nine': 261, 'nixon': 262, 'no': 263, 'not': 264, 'olympic': 265, 'on': 266, 'operations': 267, 'organization': 268, 'out': 269, 'own': 270, 'parliament': 271, 'pass': 272, 'passes': 273, 'patterson': 274, 'paul': 275, 'person': 276, 'piece': 277, 'piracy': 278, 'pirates': 279, 'plants': 280, 'played': 281, 'pounds': 282, 'powers': 283, 'practice': 284, 'presidential': 285, 'presidents': 286, 'process': 287, 'proper': 288, 'proposal': 289, 'provide': 290, 'provides': 291, 'quarter': 292, 'ranked': 293, 'ratification': 294, 'real': 295, 'recording': 296, 'recovered': 297, 'recovering': 298, 'recovery': 299, 'regular': 300, 'relationship': 301, 'remains': 302, 'replied': 303, 'rescind': 304, 'resigned': 305, 'respectively': 306, 'responsible': 307, 'resumed': 308, 'retained': 309, 'road': 310, 'role': 311, 'route': 312, 'season': 313, 'sec': 314, 'seeing': 315, 'self': 316, 'series': 317, 'serve': 318, 'several': 319, 'shot': 320, 'show': 321, 'silver': 322, 'so': 323, 'southeast': 324, 'specific': 325, 'spin': 326, 'sport': 327, 'standardised': 328, 'standing': 329, 'stating': 330, 'stealing': 331, 'studio': 332, 'sufficient': 333, 'suit': 334, 'summer': 335, 'supplied': 336, 'swamp': 337, 'swiss': 338, 'tackle': 339, 'tadman': 340, 'take': 341, 'tangled': 342, 'team': 343, 'texas': 344, 'themselves': 345, 'this': 346, 'toss': 347, 'touchdown': 348, 'traditional': 349, 'transferred': 350, 'trapped': 351, 'treaties': 352, 'tribes': 353, 'tries': 354, 'tudman': 355, 'tying': 356, 'under': 357, 'unseen': 358, 'voice': 359, 'war': 360, 'week': 361, 'weight': 362, 'what': 363, 'when': 364, 'whose': 365, 'williams': 366, 'without': 367, 'work': 368, 'working': 369, 'worse': 370, 'wrong': 371, 'wrote': 372, 'years': 373})\n"
     ]
    }
   ],
   "source": [
    "vocab = TEXT.vocab\n",
    "print(vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<pad>': 1,\n",
       " '<': 2,\n",
       " '>': 3,\n",
       " 'the': 4,\n",
       " 'unk': 5,\n",
       " 'and': 6,\n",
       " 'blank_token': 7,\n",
       " 'to': 8,\n",
       " 'a': 9,\n",
       " 'N': 10,\n",
       " 'was': 11,\n",
       " 'in': 12,\n",
       " 'of': 13,\n",
       " 'for': 14,\n",
       " 'by': 15,\n",
       " 'after': 16,\n",
       " 'his': 17,\n",
       " 'defensive': 18,\n",
       " 'is': 19,\n",
       " 'principal': 20,\n",
       " 'that': 21,\n",
       " 'who': 22,\n",
       " 'with': 23,\n",
       " \"'s\": 24,\n",
       " 'assisted': 25,\n",
       " 'but': 26,\n",
       " 'fumble': 27,\n",
       " 'gold': 28,\n",
       " 'had': 29,\n",
       " 'he': 30,\n",
       " 'james': 31,\n",
       " 'seven': 32,\n",
       " 'they': 33,\n",
       " 'were': 34,\n",
       " 'while': 35,\n",
       " 'an': 36,\n",
       " 'between': 37,\n",
       " 'coach': 38,\n",
       " 'coin': 39,\n",
       " 'dollar': 40,\n",
       " 'egypt': 41,\n",
       " 'end': 42,\n",
       " 'gallipoli': 43,\n",
       " 'great': 44,\n",
       " 'guitar': 45,\n",
       " 'henry': 46,\n",
       " 'i.': 47,\n",
       " 'italy': 48,\n",
       " 'john': 49,\n",
       " 'macedonia': 50,\n",
       " 'named': 51,\n",
       " 'off': 52,\n",
       " 'palestine': 53,\n",
       " 'player': 54,\n",
       " 'position': 55,\n",
       " 'recorded': 56,\n",
       " 'robert': 57,\n",
       " 'sir': 59,\n",
       " 'smith': 60,\n",
       " 'solo': 61,\n",
       " 'thomas': 62,\n",
       " 'total': 63,\n",
       " 'up': 64,\n",
       " 'would': 65,\n",
       " '  ': 66,\n",
       " '$': 67,\n",
       " '13th': 68,\n",
       " 'abandoned': 69,\n",
       " 'add': 70,\n",
       " 'also': 71,\n",
       " 'anglicus': 72,\n",
       " 'annular': 73,\n",
       " 'another': 74,\n",
       " 'appearance': 75,\n",
       " 'apu': 76,\n",
       " 'are': 77,\n",
       " 'area': 78,\n",
       " 'army': 79,\n",
       " 'as': 80,\n",
       " 'at': 81,\n",
       " 'attacked': 82,\n",
       " 'attacks': 83,\n",
       " 'azaria': 84,\n",
       " 'batsmen': 85,\n",
       " 'be': 86,\n",
       " 'bear': 87,\n",
       " 'began': 88,\n",
       " 'being': 89,\n",
       " 'binomial': 90,\n",
       " 'bo': 91,\n",
       " 'borgnine': 92,\n",
       " 'bowlers': 93,\n",
       " 'brooke': 94,\n",
       " 'brought': 95,\n",
       " 'camp': 96,\n",
       " 'campers': 97,\n",
       " 'chris': 98,\n",
       " 'circle': 99,\n",
       " 'commented': 100,\n",
       " 'consisting': 101,\n",
       " 'constitution': 102,\n",
       " 'correct': 103,\n",
       " 'cottage': 104,\n",
       " 'county': 105,\n",
       " 'courtney': 106,\n",
       " 'creature': 107,\n",
       " 'cricket': 108,\n",
       " 'crushing': 109,\n",
       " 'dark': 110,\n",
       " 'davis': 111,\n",
       " 'death': 112,\n",
       " 'deprecating': 113,\n",
       " 'devotion': 114,\n",
       " 'diameter': 115,\n",
       " 'doing': 116,\n",
       " 'dominate': 117,\n",
       " 'double': 118,\n",
       " 'due': 119,\n",
       " 'eagle': 120,\n",
       " 'edwin': 121,\n",
       " 'eight': 122,\n",
       " 'episode': 123,\n",
       " 'ernest': 124,\n",
       " 'even': 125,\n",
       " 'fails': 126,\n",
       " 'fight': 127,\n",
       " 'film': 128,\n",
       " 'finally': 129,\n",
       " 'finding': 130,\n",
       " 'flanders': 131,\n",
       " 'flee': 132,\n",
       " 'foot': 133,\n",
       " 'forced': 134,\n",
       " 'form': 135,\n",
       " 'four': 136,\n",
       " 'fourth': 137,\n",
       " 'friday': 138,\n",
       " 'friedrich': 139,\n",
       " 'friend': 140,\n",
       " 'from': 141,\n",
       " 'furthered': 142,\n",
       " 'garden': 143,\n",
       " 'gertrude': 144,\n",
       " 'get': 145,\n",
       " 'government': 146,\n",
       " 'greatly': 147,\n",
       " 'hands': 148,\n",
       " 'hank': 149,\n",
       " 'have': 150,\n",
       " 'hell': 151,\n",
       " 'highest': 152,\n",
       " 'him': 153,\n",
       " 'holl': 154,\n",
       " 'homer': 155,\n",
       " 'hunted': 156,\n",
       " 'idea': 157,\n",
       " 'implied': 158,\n",
       " 'influenced': 159,\n",
       " 'inswing': 160,\n",
       " 'into': 161,\n",
       " 'ironic': 162,\n",
       " 'ironically': 163,\n",
       " 'it': 164,\n",
       " 'jason': 165,\n",
       " 'jekyll': 166,\n",
       " 'johnson': 167,\n",
       " 'junior': 168,\n",
       " 'kg': 169,\n",
       " 'knife': 170,\n",
       " 'late': 171,\n",
       " 'later': 172,\n",
       " 'law': 173,\n",
       " 'lbw': 174,\n",
       " 'led': 175,\n",
       " 'legislation': 176,\n",
       " 'life': 177,\n",
       " 'line': 178,\n",
       " 'linebackers': 179,\n",
       " 'long': 180,\n",
       " 'losses': 181,\n",
       " 'making': 182,\n",
       " 'mckay': 183,\n",
       " 'meanwhile': 184,\n",
       " 'men': 185,\n",
       " 'mountain': 186,\n",
       " 'much': 187,\n",
       " 'name': 188,\n",
       " 'ned': 189,\n",
       " 'neglected': 190,\n",
       " 'neither': 191,\n",
       " 'nelson': 192,\n",
       " 'no': 193,\n",
       " 'not': 194,\n",
       " 'one': 195,\n",
       " 'or': 196,\n",
       " 'other': 197,\n",
       " 'out': 198,\n",
       " 'own': 199,\n",
       " 'parliament': 200,\n",
       " 'pass': 201,\n",
       " 'patterson': 202,\n",
       " 'paul': 203,\n",
       " 'person': 204,\n",
       " 'piece': 205,\n",
       " 'piracy': 206,\n",
       " 'pirates': 207,\n",
       " 'plants': 208,\n",
       " 'played': 209,\n",
       " 'pounds': 210,\n",
       " 'powers': 211,\n",
       " 'practice': 212,\n",
       " 'presidential': 213,\n",
       " 'proper': 214,\n",
       " 'proposal': 215,\n",
       " 'provide': 216,\n",
       " 'provides': 217,\n",
       " 'quarter': 218,\n",
       " 'real': 219,\n",
       " 'recording': 220,\n",
       " 'recovering': 221,\n",
       " 'recovery': 222,\n",
       " 'relationship': 223,\n",
       " 'replied': 224,\n",
       " 'resigned': 225,\n",
       " 'respectively': 226,\n",
       " 'responsible': 227,\n",
       " 'resumed': 228,\n",
       " 'road': 229,\n",
       " 'role': 230,\n",
       " 'route': 231,\n",
       " 'sec': 232,\n",
       " 'second': 233,\n",
       " 'seeing': 234,\n",
       " 'self': 235,\n",
       " 'series': 236,\n",
       " 'serve': 237,\n",
       " 'shot': 238,\n",
       " 'show': 239,\n",
       " 'silver': 240,\n",
       " 'so': 241,\n",
       " 'specific': 242,\n",
       " 'spin': 243,\n",
       " 'sport': 244,\n",
       " 'standardised': 245,\n",
       " 'standing': 246,\n",
       " 'stating': 247,\n",
       " 'stealing': 248,\n",
       " 'studio': 249,\n",
       " 'summer': 250,\n",
       " 'supplied': 251,\n",
       " 'swamp': 252,\n",
       " 'swiss': 253,\n",
       " 'tackles': 254,\n",
       " 'tadman': 255,\n",
       " 'take': 256,\n",
       " 'tangled': 257,\n",
       " 'texas': 258,\n",
       " 'themselves': 259,\n",
       " 'this': 260,\n",
       " 'three': 261,\n",
       " 'toss': 262,\n",
       " 'touchdown': 263,\n",
       " 'traditional': 264,\n",
       " 'transferred': 265,\n",
       " 'trapped': 266,\n",
       " 'tries': 267,\n",
       " 'tudman': 268,\n",
       " 'two': 269,\n",
       " 'unseen': 270,\n",
       " 'voice': 271,\n",
       " 'week': 272,\n",
       " 'weight': 273,\n",
       " 'what': 274,\n",
       " 'when': 275,\n",
       " 'which': 276,\n",
       " 'whose': 277,\n",
       " 'williams': 278,\n",
       " 'work': 279,\n",
       " 'working': 280,\n",
       " 'worse': 281,\n",
       " 'wrong': 282,\n",
       " 'wrote': 283}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i:vocab.stoi[i] for i in vocab.stoi if i!='sentence'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In torchvision and PyTorch, the processing and batching of\n",
    "# data is handled by DataLoaders. For some reason, torchtext\n",
    "# has renamed the objects that do the exact same thing to\n",
    "# Iterators. The basic functionality is the same\n",
    "\n",
    "train_iter, test_iter = Iterator.splits(\n",
    "        (train, test),\n",
    "    \n",
    "        # (91270, 10153) means 91270 for train and 10153 for test,\n",
    "        # the number of examples in each\n",
    "        # That is, we only want to create one \"batch\" for each\n",
    "        # as we are only doing this process in TorchText to convert\n",
    "        # our data into a PyTorch tensor object to be passed around\n",
    "        # the matching networks program in the same way the\n",
    "        # vision data was passed around in a numpy array\n",
    "        # The matching networks program already takes care\n",
    "        # of batching and we don;t want to distrub things too much\n",
    "    \n",
    "        batch_sizes=(20,5),sort_key=None, device=None, batch_size_fn=None, repeat=False, shuffle=None, sort=None, sort_within_batch=None)\n",
    "\n",
    "# train_iter, test_iter = Iterator(dataset=train, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the batch\n",
    "\n",
    "# batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_iter.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, the iterator returns a custom datatype\n",
    "# called torchtext.data.Batch.\n",
    "# we’ll convert the batch to a tuple in the form\n",
    "# (x, y) where x is the label tensor\n",
    "# and y is the sentence\n",
    "\n",
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_var):\n",
    "        \n",
    "        self.dl, self.x_var, self.y_var = dl, x_var, y_var # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            y = getattr(batch, self.y_var) # we assume only one input in this wrapper\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"label\", \"sentence\")\n",
    "test_dl = BatchWrapper(test_iter, \"label\", \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 5, 1, 2, 1, 2, 2]), tensor([[243,  19,  17,  ..., 130,   9,  11],\n",
      "        [  2, 355, 238,  ..., 352,   7, 102],\n",
      "        [  6,  29,  52,  ...,  65,   9,   4],\n",
      "        ...,\n",
      "        [ 10,   1,   1,  ...,   1,   1,   1],\n",
      "        [322,   1,   1,  ...,   1,   1,   1],\n",
      "        [ 53,   1,   1,  ...,   1,   1,   1]]))\n"
     ]
    }
   ],
   "source": [
    "X = next(train_dl.__iter__())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 4, 4, 1, 4, 1, 4, 4, 1])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_0 = X[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_1 = X[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Y_0.argsort()\n",
    "Y_0p = Y_0[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[275  16 260 183  30 255 106 278  18  10]\n",
      " [  4 268  11   2 190  29   2  71 178   6]\n",
      " [244  24   2   7   8  32   5  56  38  10]\n",
      " [228 112   7   3  70  61   3 122  91   2]\n",
      " [ 12  12   3  17   9   2  11  61 111   5]\n",
      " [ 10  10   8 176 242   7  51   2 225   3]\n",
      " [ 85   4   9   8 188   3   4   7  17  34]\n",
      " [ 34 102  99 216   8 261 232   3  55 179]\n",
      " [198  11  78  14 135  25  18   4   8 203]\n",
      " [ 13   2  23   9   9 254  54 233 237 192]\n",
      " [212   7   9 118 214   6  13 152  80   6]\n",
      " [  6   3  32 120  90  56   4  63   4   2]\n",
      " [  4   6 133  67  26   9 272  14  18   5]\n",
      " [  2 187 115  10 195  18  16   4   2   3]\n",
      " [  7  13  12  28  11 263 182 207   7   2]\n",
      " [  3   4  10  39 251  16  32   6   3   5]\n",
      " [174 213   6   6  12 221  63   9  38   3]\n",
      " [173 211   4 283  10  98   2 134  14  22]\n",
      " [209  34 273   8  15 167   7  27 258  29]\n",
      " [161 265  13 202 139  24   3   1   1  10]\n",
      " [  4   8   4  22 154  27  23   1   1   6]\n",
      " [148   4 238 224  47 171 136   1   1  10]\n",
      " [ 13 200  11 247   2  12  14   1   1   2]\n",
      " [ 52   6 245  21   5   4 181   1   1   7]\n",
      " [243   4   8   4   3 137   9   1   1   3]\n",
      " [  6 146  10  73 276 218  27   1   1 226]\n",
      " [160   1 210  28  11   1 222   1   1   1]\n",
      " [ 93   1  10  40 172   1   6   1   1   1]\n",
      " [ 22   1  10  65   2   1 269   1   1   1]\n",
      " [ 88   1 169 194   7   1 201   1   1   1]\n",
      " [  8   1   1 279   3   1   2   1   1   1]\n",
      " [117   1   1   6   8   1   5   1   1   1]\n",
      " [105   1   1 191  47   1   3   1   1   1]\n",
      " [108   1   1  65  72   1   1   1   1   1]\n",
      " [  1   1   1  74   1   1   1   1   1   1]\n",
      " [  1   1   1 215   1   1   1   1   1   1]\n",
      " [  1   1   1   8   1   1   1   1   1   1]\n",
      " [  1   1   1 150   1   1   1   1   1   1]\n",
      " [  1   1   1  40   1   1   1   1   1   1]\n",
      " [  1   1   1 205   1   1   1   1   1   1]\n",
      " [  1   1   1 101   1   1   1   1   1   1]\n",
      " [  1   1   1  13   1   1   1   1   1   1]\n",
      " [  1   1   1   9   1   1   1   1   1   1]\n",
      " [  1   1   1  28   1   1   1   1   1   1]\n",
      " [  1   1   1   2   1   1   1   1   1   1]\n",
      " [  1   1   1   5   1   1   1   1   1   1]\n",
      " [  1   1   1   3   1   1   1   1   1   1]\n",
      " [  1   1   1  12   1   1   1   1   1   1]\n",
      " [  1   1   1   9   1   1   1   1   1   1]\n",
      " [  1   1   1 240   1   1   1   1   1   1]\n",
      " [  1   1   1  39   1   1   1   1   1   1]]\n"
     ]
    }
   ],
   "source": [
    "Y_1p = Y_1[:,p]\n",
    "print(Y_1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 1, 1, 1, 1, 4, 4, 4, 4, 4], dtype=int64), array([[275,  16, 260, 183,  30, 255, 106, 278,  18,  10],\n",
      "       [  4, 268,  11,   2, 190,  29,   2,  71, 178,   6],\n",
      "       [244,  24,   2,   7,   8,  32,   5,  56,  38,  10],\n",
      "       [228, 112,   7,   3,  70,  61,   3, 122,  91,   2],\n",
      "       [ 12,  12,   3,  17,   9,   2,  11,  61, 111,   5],\n",
      "       [ 10,  10,   8, 176, 242,   7,  51,   2, 225,   3],\n",
      "       [ 85,   4,   9,   8, 188,   3,   4,   7,  17,  34],\n",
      "       [ 34, 102,  99, 216,   8, 261, 232,   3,  55, 179],\n",
      "       [198,  11,  78,  14, 135,  25,  18,   4,   8, 203],\n",
      "       [ 13,   2,  23,   9,   9, 254,  54, 233, 237, 192],\n",
      "       [212,   7,   9, 118, 214,   6,  13, 152,  80,   6],\n",
      "       [  6,   3,  32, 120,  90,  56,   4,  63,   4,   2],\n",
      "       [  4,   6, 133,  67,  26,   9, 272,  14,  18,   5],\n",
      "       [  2, 187, 115,  10, 195,  18,  16,   4,   2,   3],\n",
      "       [  7,  13,  12,  28,  11, 263, 182, 207,   7,   2],\n",
      "       [  3,   4,  10,  39, 251,  16,  32,   6,   3,   5],\n",
      "       [174, 213,   6,   6,  12, 221,  63,   9,  38,   3],\n",
      "       [173, 211,   4, 283,  10,  98,   2, 134,  14,  22],\n",
      "       [209,  34, 273,   8,  15, 167,   7,  27, 258,  29],\n",
      "       [161, 265,  13, 202, 139,  24,   3,   1,   1,  10],\n",
      "       [  4,   8,   4,  22, 154,  27,  23,   1,   1,   6],\n",
      "       [148,   4, 238, 224,  47, 171, 136,   1,   1,  10],\n",
      "       [ 13, 200,  11, 247,   2,  12,  14,   1,   1,   2],\n",
      "       [ 52,   6, 245,  21,   5,   4, 181,   1,   1,   7],\n",
      "       [243,   4,   8,   4,   3, 137,   9,   1,   1,   3],\n",
      "       [  6, 146,  10,  73, 276, 218,  27,   1,   1, 226],\n",
      "       [160,   1, 210,  28,  11,   1, 222,   1,   1,   1],\n",
      "       [ 93,   1,  10,  40, 172,   1,   6,   1,   1,   1],\n",
      "       [ 22,   1,  10,  65,   2,   1, 269,   1,   1,   1],\n",
      "       [ 88,   1, 169, 194,   7,   1, 201,   1,   1,   1],\n",
      "       [  8,   1,   1, 279,   3,   1,   2,   1,   1,   1],\n",
      "       [117,   1,   1,   6,   8,   1,   5,   1,   1,   1],\n",
      "       [105,   1,   1, 191,  47,   1,   3,   1,   1,   1],\n",
      "       [108,   1,   1,  65,  72,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,  74,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1, 215,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,   8,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1, 150,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,  40,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1, 205,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1, 101,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,  13,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,   9,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,  28,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,   2,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,   5,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,   3,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,  12,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,   9,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1, 240,   1,   1,   1,   1,   1,   1],\n",
      "       [  1,   1,   1,  39,   1,   1,   1,   1,   1,   1]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "Yp = (Y_0p, Y_1p)\n",
    "print(Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 1, 4, 4, 1, 4, 1, 4, 4, 1], dtype=int64), array([[275,  16, 255, 106, 260, 278, 183,  18,  10,  30],\n",
      "       [  4, 268,  29,   2,  11,  71,   2, 178,   6, 190],\n",
      "       [244,  24,  32,   5,   2,  56,   7,  38,  10,   8],\n",
      "       [228, 112,  61,   3,   7, 122,   3,  91,   2,  70],\n",
      "       [ 12,  12,   2,  11,   3,  61,  17, 111,   5,   9],\n",
      "       [ 10,  10,   7,  51,   8,   2, 176, 225,   3, 242],\n",
      "       [ 85,   4,   3,   4,   9,   7,   8,  17,  34, 188],\n",
      "       [ 34, 102, 261, 232,  99,   3, 216,  55, 179,   8],\n",
      "       [198,  11,  25,  18,  78,   4,  14,   8, 203, 135],\n",
      "       [ 13,   2, 254,  54,  23, 233,   9, 237, 192,   9],\n",
      "       [212,   7,   6,  13,   9, 152, 118,  80,   6, 214],\n",
      "       [  6,   3,  56,   4,  32,  63, 120,   4,   2,  90],\n",
      "       [  4,   6,   9, 272, 133,  14,  67,  18,   5,  26],\n",
      "       [  2, 187,  18,  16, 115,   4,  10,   2,   3, 195],\n",
      "       [  7,  13, 263, 182,  12, 207,  28,   7,   2,  11],\n",
      "       [  3,   4,  16,  32,  10,   6,  39,   3,   5, 251],\n",
      "       [174, 213, 221,  63,   6,   9,   6,  38,   3,  12],\n",
      "       [173, 211,  98,   2,   4, 134, 283,  14,  22,  10],\n",
      "       [209,  34, 167,   7, 273,  27,   8, 258,  29,  15],\n",
      "       [161, 265,  24,   3,  13,   1, 202,   1,  10, 139],\n",
      "       [  4,   8,  27,  23,   4,   1,  22,   1,   6, 154],\n",
      "       [148,   4, 171, 136, 238,   1, 224,   1,  10,  47],\n",
      "       [ 13, 200,  12,  14,  11,   1, 247,   1,   2,   2],\n",
      "       [ 52,   6,   4, 181, 245,   1,  21,   1,   7,   5],\n",
      "       [243,   4, 137,   9,   8,   1,   4,   1,   3,   3],\n",
      "       [  6, 146, 218,  27,  10,   1,  73,   1, 226, 276],\n",
      "       [160,   1,   1, 222, 210,   1,  28,   1,   1,  11],\n",
      "       [ 93,   1,   1,   6,  10,   1,  40,   1,   1, 172],\n",
      "       [ 22,   1,   1, 269,  10,   1,  65,   1,   1,   2],\n",
      "       [ 88,   1,   1, 201, 169,   1, 194,   1,   1,   7],\n",
      "       [  8,   1,   1,   2,   1,   1, 279,   1,   1,   3],\n",
      "       [117,   1,   1,   5,   1,   1,   6,   1,   1,   8],\n",
      "       [105,   1,   1,   3,   1,   1, 191,   1,   1,  47],\n",
      "       [108,   1,   1,   1,   1,   1,  65,   1,   1,  72],\n",
      "       [  1,   1,   1,   1,   1,   1,  74,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1, 215,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,   8,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1, 150,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,  40,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1, 205,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1, 101,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,  13,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,   9,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,  28,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,   2,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,   5,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,   3,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,  12,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,   9,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1, 240,   1,   1,   1],\n",
      "       [  1,   1,   1,   1,   1,   1,  39,   1,   1,   1]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "Y = (Y_0, Y_1)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(train_dl.__iter__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(train_dl.__iter__())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(next(train_dl.__iter__())[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(train_dl.__iter__())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49,  98,  52, 101,  11,  22,   4,   8,  10, 111,  44],\n",
      "        [ 58,   1,   2,  16,  39,   4,  30,  71,   6,  21,   8],\n",
      "        [  4,   1,   7,  12,   4,  30,  68,  23,  10,  31,  27],\n",
      "        [ 93,   1,   3,  18,  55,  27,  50,  46,   2,  24,  22],\n",
      "        [ 96,   1, 108,   2,  54,  38,  61,  53,   7,  18,  75],\n",
      "        [ 13,   1,  78,   5,  89,  81,   8,  94,   3,   2,  16],\n",
      "        [ 10,   1,   4,   3,  56,   2,  43,  66, 110,   5,  29],\n",
      "        [  2,   1,  97,  19,  79,   5,   2,  87,  72,   3,  17],\n",
      "        [  5,   1,   8,  42,  11,   3,   7, 104,  85,   4,  13],\n",
      "        [  3,   1,  17,  33,   2,   4,   3,  99,  80,  32,  24],\n",
      "        [ 48,   1,  28,   6,   5,  32,  76,  41,   6,  64,   2],\n",
      "        [107,   1,   4,  31,   3,  77,  36,   4,   2,  34,   5],\n",
      "        [ 25,   1, 109,  14,   9,  28,  21,   8,   7,   9,   3],\n",
      "        [ 84,   1,  20,   8,  26,  40,  47,   2,   3,   4,   6],\n",
      "        [  6,   1,  74, 105,   1,  17,  19,   5,   2,  86,  35],\n",
      "        [ 90,   1,  12,  20,   1,  11,  18,   3,   7,   6,  13],\n",
      "        [ 19,   1,  34,  91,   1,   4,   2,  23,   3,  14,  12],\n",
      "        [ 62,   1,   2,  51,   1,  63,   5,   9,  36,  59,   1],\n",
      "        [  1,   1,   5,  69,   1,  67,   3, 103,  16,  15,   1],\n",
      "        [  1,   1,   3,  37,   1,  12, 106,   1,  10,   1,   1],\n",
      "        [  1,   1,  13,  15,   1,   2,  65,   1,   6,   1,   1],\n",
      "        [  1,   1,  25,  70,   1,   7,   9,   1,  10,   1,   1],\n",
      "        [  1,   1,   9,  11,   1,   3,  57,   1,   2,   1,   1],\n",
      "        [  1,   1,  73,   4,   1,  33,  45,   1,   5,   1,   1],\n",
      "        [  1,   1,  14,  60,   1,   6,  82,   1,   3,   1,   1],\n",
      "        [  1,   1,  15,  88,   1,  29,   4,   1,  95,   1,   1],\n",
      "        [  1,   1,  92,   1,   1, 100, 102,   1,   1,   1,   1],\n",
      "        [  1,   1,   6,   1,   1,   9,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,  35,   1,   1,  26,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,  83,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   7,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   3,   1,   1,   1,   1,   1,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "print(next(train_dl.__iter__())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 11])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a 2D tensor, but self is 1D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-543000525590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: t() expects a 2D tensor, but self is 1D"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "\n",
    "Y_train = next(train_dl.__iter__())[0]\n",
    "X_train = next(train_dl.__iter__())[1]\n",
    "Y_test = next(test_dl.__iter__())[0]\n",
    "X_test = next(test_dl.__iter__())[1]\n",
    "\n",
    "\n",
    "X_train = X_train.t()\n",
    "Y_train = Y_train.t()\n",
    "X_test = X_test.t()\n",
    "Y_test = Y_test.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "\n",
    "# We convert the tensors to numpy arrays\n",
    "# for the Matching Networks code so we  don't\n",
    "# have to change everything that was for numpy arrays\n",
    "# to PyTorch tensors. We could could then convert\n",
    "# back to Tensors when we need them\n",
    "\n",
    "# Cell incomplete, issue with size of train. Should be 90,000, not 9,000 long\n",
    "\n",
    "X_train = X_train.reshape()\n",
    "Y_train = Y_train.reshape()\n",
    "X_test = X_test.reshape()\n",
    "Y_test = Y_test.reshape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to numpy array\n",
    "\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('Y_train.npy', Y_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('Y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
